{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np \nfrom scipy.sparse import csr_matrix\n!pip install pymorphy2\nimport pymorphy2\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-17T14:57:14.656896Z","iopub.execute_input":"2021-12-17T14:57:14.657203Z","iopub.status.idle":"2021-12-17T14:57:22.124725Z","shell.execute_reply.started":"2021-12-17T14:57:14.657148Z","shell.execute_reply":"2021-12-17T14:57:22.123897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:22.130931Z","iopub.execute_input":"2021-12-17T14:57:22.131626Z","iopub.status.idle":"2021-12-17T14:57:22.147855Z","shell.execute_reply.started":"2021-12-17T14:57:22.131579Z","shell.execute_reply":"2021-12-17T14:57:22.147039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = open(\"../input/doc-cls/corp0.txt\", \"r\")\nfiledata = file.readlines()\nprint(\"Размер dataset\", len(filedata))\n\n# dictionary = file.get_feature_names()\n# print(\" Размер словаря корпуса до предварительной обработки.\", len(dictionary))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:22.150326Z","iopub.execute_input":"2021-12-17T14:57:22.15059Z","iopub.status.idle":"2021-12-17T14:57:22.202885Z","shell.execute_reply.started":"2021-12-17T14:57:22.150551Z","shell.execute_reply":"2021-12-17T14:57:22.202155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Подготовка набора данных.\n\nВариант токенизации:\n\n·     ограничиваем максимальную длину документа (по числу словоформ);\n\n·     приводим в нижний регистр;\n\n·     иностранные слова заменяем на FRGN;\n\n·     элементы с цифрами заменяем на NUMB;\n\n·     удаляем знаки препинания;\n\n·     Лемматизация русских слов. Её цель - сокращение словаря;\n\n·     разделение данных и меток в разные файлы (x.txt, y.txt).\n\nЛемматизация – это приведение словоформы к лемме, или нормальной форме. ","metadata":{}},{"cell_type":"code","source":"#Проверка. Пример предварительной обработки корпуса с выводом результатов для нескольких элементов датасета\n\ndataset = filedata [35:39]#[54:60]\nprint(\"Dataset \\n\",dataset, \"\\n\")\nprint(\"Размер dataset\", len(dataset))\n\n#делаем все слова нижнего регистра\na = [x.lower() for x in dataset]\n\n# иностранные слова заменяем на FRGN; элементы с цифрами заменяем на NUMB\nb = []\nn = len(a)\nfor i in range (n):\n    b.append(a[i].split(\" \"))\n    for j in range (0, len(b[i])):\n        if (re.search('[a-z]', b[i][j])):\n            b[i][j]= \"frgn\"#\"FRGN\"\n        if (re.search('[0-9]', b[i][j])):\n            b[i][j]= \"numb\"#\"NUMB\"\n    b[i]=\" \".join(b[i])\n    \n#print(b)\n\nout = []\nfor x in b:  \n     out.append (re.sub(r'[^\\w\\s]','', x))\n\ndataset = out\nprint(\"--------------------------------------\")\nprint(\"dataset после подготовки данных\")\nprint(dataset)\n\n  \n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:22.204957Z","iopub.execute_input":"2021-12-17T14:57:22.205143Z","iopub.status.idle":"2021-12-17T14:57:22.217928Z","shell.execute_reply.started":"2021-12-17T14:57:22.20512Z","shell.execute_reply":"2021-12-17T14:57:22.217168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# лемматизация\n# Заменяет  слова леммами \ndef to_normal_form(morph, s): \n    s2 = s.split() # Список слов предложения s \n    s = '' \n    for w in s2: \n        w = morph.parse(w)[0].normal_form \n        s += (' ' + w) \n    return s.lstrip() \nmorph = pymorphy2.MorphAnalyzer() \nfor i in range (len(dataset)):\n    dataset[i]=to_normal_form(morph, dataset[i])\nprint (dataset)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:22.219534Z","iopub.execute_input":"2021-12-17T14:57:22.220021Z","iopub.status.idle":"2021-12-17T14:57:22.528275Z","shell.execute_reply.started":"2021-12-17T14:57:22.219983Z","shell.execute_reply":"2021-12-17T14:57:22.527512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:22.529631Z","iopub.execute_input":"2021-12-17T14:57:22.530045Z","iopub.status.idle":"2021-12-17T14:57:22.538596Z","shell.execute_reply.started":"2021-12-17T14:57:22.530006Z","shell.execute_reply":"2021-12-17T14:57:22.537672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = out\n\n#извлекаем метки\nprint(\"--------------------------------------\")\ny_data = []\nn = len (dataset)\nfor i in range (n):\n    y_words = dataset[i].split(\" \")\n    y_data.append(y_words[0])\n    \n#print(\"Разбиение предложения по словам y_words\", y_words,\"\\n\")\nprint(\"Метки y_label, \\n\", y_data,\"\\n\")\n#извлекаем предложения без меток\nprint(\"--------------------------------------\")\nx_data = []\nn = len (dataset)\nfor i in range (n):\n    ind_begin = dataset[i].index(\" \")\n    ind_begin +=1\n    x_data.append(dataset[i][ind_begin:])\n    \nprint(\"Предложения без меток: \\n\", x_data,\"\\n\")\n\nf_x = open(\"f_x.txt\", \"w\")\nf_y = open(\"f_y.txt\", \"w\")\n\nnp.savetxt(f_x, x_data, fmt='%s')\nnp.savetxt(f_y, y_data, fmt='%s')\n\nf_x.close()\nf_y.close() \n\n\n# x_data = np.array(x_data)\n# y_data = np.array(y_data)\n\n# for  line in x_data:\n#      f_x.write(line+\"\\n\")\n# for  line in y_data:\n#      f_y.write(line+\"\\n\")\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:22.540355Z","iopub.execute_input":"2021-12-17T14:57:22.540614Z","iopub.status.idle":"2021-12-17T14:57:22.552209Z","shell.execute_reply.started":"2021-12-17T14:57:22.540581Z","shell.execute_reply":"2021-12-17T14:57:22.551498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Основная программа","metadata":{}},{"cell_type":"markdown","source":"### Проверка. Пример предварительной обработки корпуса с выводом результатов для нескольких элементов датасета","metadata":{}},{"cell_type":"code","source":"# #Проверка. Пример предварительной обработки корпуса с выводом результатов для нескольких элементов датасета\n# dataset = filedata \n# #print(\"Dataset \\n\",dataset, \"\\n\")\n# print(\"Размер dataset\", len(dataset))\n\n# #делаем все слова нижнего регистра\n# a = [x.lower() for x in dataset]\n\n# # иностранные слова заменяем на FRGN; элементы с цифрами заменяем на NUMB\n# b = []\n# n = len(a)\n# for i in range (n):\n#     b.append(a[i].split(\" \"))\n#     for j in range (0, len(b[i])):\n#         if (re.search('[a-z]', b[i][j])):\n#             b[i][j]= \"frgn\"#\"FRGN\"\n#         if (re.search('[0-9]', b[i][j])):\n#             b[i][j]= \"numb\"#\"NUMB\"\n#     b[i]=\" \".join(b[i])\n    \n# #print(b)\n# out = []\n# for x in b:  \n#      out.append (re.sub(r'[^\\w\\s]','', x))\n\n# dataset = out\n# # print(\"--------------------------------------\")\n# # print(\"dataset после подготовки данных\")\n# # print(dataset)\n# print(\"Результат получен\")\n\n  \n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:22.553755Z","iopub.execute_input":"2021-12-17T14:57:22.554244Z","iopub.status.idle":"2021-12-17T14:57:22.561976Z","shell.execute_reply.started":"2021-12-17T14:57:22.554205Z","shell.execute_reply":"2021-12-17T14:57:22.561182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  лемматизация","metadata":{}},{"cell_type":"code","source":"# # лемматизация\n# # Заменяет  слова леммами \n# def to_normal_form(morph, s): \n#     s2 = s.split() # Список слов предложения s \n#     s = '' \n#     for w in s2: \n#         w = morph.parse(w)[0].normal_form \n#         s += (' ' + w) \n#     return s.lstrip() \n# morph = pymorphy2.MorphAnalyzer() \n# for i in range (len(dataset)):\n#     dataset[i]=to_normal_form(morph, dataset[i])\n# #print (dataset)\n# print(\"Результат получен\")\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:22.562895Z","iopub.execute_input":"2021-12-17T14:57:22.565401Z","iopub.status.idle":"2021-12-17T14:57:22.574051Z","shell.execute_reply.started":"2021-12-17T14:57:22.565359Z","shell.execute_reply":"2021-12-17T14:57:22.573258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset = out\n\n# #извлекаем метки\n# y_data = []\n# n = len (dataset)\n# for i in range (n):\n#     y_words = dataset[i].split(\" \")\n#     y_data.append(y_words[0])\n    \n# #print(\"Разбиение предложения по словам y_words\", y_words,\"\\n\")\n# #print(\"Метки y_label, \\n\", y_data,\"\\n\")\n# #извлекаем предложения без меток\n# x_data = []\n# n = len (dataset)\n# for i in range (n):\n#     ind_begin = dataset[i].index(\" \")\n#     #ind_end = dataset[i].index(\"\\n\")\n#     ind_begin +=1\n#     #x_data.append(dataset[i][ind_begin:ind_end])\n#     x_data.append(dataset[i][ind_begin:])\n    \n# #print(\"Предложения без меток: \\n\", x_data,\"\\n\")\n\n# f_x = open(\"f_x.txt\", \"w\")\n# f_y = open(\"f_y.txt\", \"w\")\n\n# np.savetxt(f_x, x_data, fmt='%s')\n# np.savetxt(f_y, y_data, fmt='%s')\n\n# f_x.close()\n# f_y.close() \n# print(\"Данные записаны в файлы\")\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:22.575443Z","iopub.execute_input":"2021-12-17T14:57:22.575905Z","iopub.status.idle":"2021-12-17T14:57:22.583399Z","shell.execute_reply.started":"2021-12-17T14:57:22.575869Z","shell.execute_reply":"2021-12-17T14:57:22.582659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Делим набор данных на обучающее  и проверочное множества.","metadata":{}},{"cell_type":"code","source":"x_file = open(\"../input/new-data/f_x_dat.txt\", \"r\")\nx_data = x_file.readlines()\nprint(\"Размер dataset\", len(x_data))\n\ny_file = open(\"../input/new-data/f_y_dat.txt\", \"r\")\ny_data = y_file.readlines()\nprint(\"Кол-во меток\", len(y_data))\n\n\n#<загрузка  данных и меток в списки (массивы) x, y>\nk_split = 0.2\n\nx = x_data\ny = y_data\n\nfrom sklearn.preprocessing import LabelEncoder\nimport keras\nlencoder = LabelEncoder()\nlencoder.fit(y_data)\n\nx_trn, x_vl, y_trn, y_vl = train_test_split(x, y, test_size = k_split, shuffle = True)\nprint( \"кол-во данных в x_trn\", len(x_trn))\nprint( \"кол-во данных в x_ vl\", len(x_vl))\n#print(x_trn,\"\\n\", x_vl,\"\\n\", y_trn,\"\\n\", y_vl)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:22.584701Z","iopub.execute_input":"2021-12-17T14:57:22.585158Z","iopub.status.idle":"2021-12-17T14:57:24.315277Z","shell.execute_reply.started":"2021-12-17T14:57:22.585121Z","shell.execute_reply":"2021-12-17T14:57:24.314484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Векторизация.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer \nvec = CountVectorizer(token_pattern = '\\w+', binary = False) # Если задать ,binary = True то векторы, представляющие строки текста, будут бинарными\nlen_trn = len(x_trn) \nx_trn.extend(x_vl) # Объединяем x_trn и x_vl и получаем полный корпус \nx_trn = vec.fit_transform(x_trn) \nx_vl = x_trn[len_trn:] \nx_trn = x_trn[:len_trn] \n# Преобразуем разреженные матрицы в массивы \nx_trn = np.float32(x_trn.toarray()) \nx_vl = np.float32(x_vl.toarray())\nprint(x_trn.shape)\nprint(x_vl.shape)\n\nprint(len(vec.get_feature_names()))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:24.316475Z","iopub.execute_input":"2021-12-17T14:57:24.316744Z","iopub.status.idle":"2021-12-17T14:57:25.681231Z","shell.execute_reply.started":"2021-12-17T14:57:24.316708Z","shell.execute_reply":"2021-12-17T14:57:25.679706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:25.684151Z","iopub.execute_input":"2021-12-17T14:57:25.684394Z","iopub.status.idle":"2021-12-17T14:57:25.689016Z","shell.execute_reply.started":"2021-12-17T14:57:25.684364Z","shell.execute_reply":"2021-12-17T14:57:25.687411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SGDClassifier.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier \ndoc_clf = SGDClassifier(loss = 'hinge', max_iter = 1000, tol = 1e-3) \n\nstart_time = time.time()\n#print(y_trn)\ndoc_clf.fit(x_trn, y_trn) # Обучение классификатора \nend_time = time.time() - start_time\nprint(\"Время выполнения :  {:.8f} секунд \" .format(end_time) )\nprint('Оценка точности классификации') \nscore = doc_clf.score(x_vl, y_vl) \nprint('Точность на проверочном множестве:', round(score, 4)) \nscore = doc_clf.score(x_trn, y_trn) \nprint('Точность на обучающем множестве:', round(score, 4))\n\npredictions = doc_clf.predict(x_vl)\nprint(classification_report(lencoder.transform(y_vl), lencoder.transform(predictions), digits = 4, target_names = lencoder.classes_))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:57:25.690502Z","iopub.execute_input":"2021-12-17T14:57:25.690748Z","iopub.status.idle":"2021-12-17T14:58:09.82686Z","shell.execute_reply.started":"2021-12-17T14:57:25.690716Z","shell.execute_reply":"2021-12-17T14:58:09.824295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LogisticRegression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression \ndoc_clf = LogisticRegression(solver = 'lbfgs', # newton-cg \n                             max_iter = 500, multi_class = 'auto') \nstart_time = time.time()\ndoc_clf.fit(x_trn, y_trn) # Обучение классификатора \nend_time = time.time() - start_time\nprint(\"Время выполнения :  {:.8f} секунд \" .format(end_time) )\nprint('Оценка точности классификации') \nscore = doc_clf.score(x_vl, y_vl) \nprint('Точность на проверочном множестве:', round(score, 4)) \nscore = doc_clf.score(x_trn, y_trn) \nprint('Точность на обучающем множестве:', round(score, 4))\n\npredictions = doc_clf.predict(x_vl)\nprint(classification_report(lencoder.transform(y_vl), lencoder.transform(predictions), digits = 4, target_names = lencoder.classes_))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T14:58:09.828174Z","iopub.execute_input":"2021-12-17T14:58:09.82862Z","iopub.status.idle":"2021-12-17T15:00:51.730763Z","shell.execute_reply.started":"2021-12-17T14:58:09.828582Z","shell.execute_reply":"2021-12-17T15:00:51.729909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#---------------------\nfrom sklearn.svm import SVC \ndoc_clf_svc = SVC()\n#doc_clf = SGDClassifier(loss = 'hinge', max_iter = 1000, tol = 1e-3) \n\nstart_time = time.time()\n#print(y_trn)\ndoc_clf_svc.fit(x_trn, y_trn) # Обучение классификатора \nend_time = time.time() - start_time\nprint(\"Время выполнения :  {:.8f} секунд \" .format(end_time) )\nprint('Оценка точности классификации') \nscore = doc_clf_svc.score(x_vl, y_vl) \nprint('Точность на проверочном множестве:', round(score, 4)) \nscore = doc_clf_svc.score(x_trn, y_trn) \nprint('Точность на обучающем множестве:', round(score, 4))\n\npredictions = doc_clf.predict(x_vl)\nprint(classification_report(lencoder.transform(y_vl), lencoder.transform(predictions), digits = 4, target_names = lencoder.classes_))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:00:51.73515Z","iopub.execute_input":"2021-12-17T15:00:51.735407Z","iopub.status.idle":"2021-12-17T15:15:24.427737Z","shell.execute_reply.started":"2021-12-17T15:00:51.735374Z","shell.execute_reply":"2021-12-17T15:15:24.426816Z"},"trusted":true},"execution_count":null,"outputs":[]}]}